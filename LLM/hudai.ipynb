{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
      "  Downloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting alembic>=0.6.2 (from dataset)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting banal>=1.0.1 (from dataset)\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting Mako (from alembic>=0.6.2->dataset)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/codespace/.local/lib/python3.12/site-packages (from alembic>=0.6.2->dataset) (4.12.2)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy<2.0.0,>=1.3.2->dataset)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/codespace/.local/lib/python3.12/site-packages (from Mako->alembic>=0.6.2->dataset) (3.0.2)\n",
      "Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Downloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: banal, Mako, greenlet, sqlalchemy, alembic, dataset\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 banal-1.0.6 dataset-1.6.2 greenlet-3.1.1 sqlalchemy-1.4.54\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          BertConfig, BertForMaskedLM, BertTokenizer,\n",
    "                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "                          RobertaConfig, RobertaForMaskedLM, RobertaTokenizer,\n",
    "                          DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, logger, file_type='train', block_size=1024):\n",
    "        if args.local_rank==-1:\n",
    "            local_rank=0\n",
    "            world_size=1\n",
    "        else:\n",
    "            local_rank=args.local_rank\n",
    "            world_size=torch.distributed.get_world_size()\n",
    "\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "        cached_file = os.path.join(args.output_dir, file_type+\"_langs_%s\"%(args.langs)+\"_blocksize_%d\"%(block_size)+\"_wordsize_%d\"%(world_size)+\"_rank_%d\"%(local_rank))\n",
    "        if os.path.exists(cached_file) and not args.overwrite_cache:\n",
    "            if file_type == 'train':\n",
    "                logger.warning(\"Loading features from cached file %s\", cached_file)\n",
    "            with open(cached_file, 'rb') as handle:\n",
    "                self.inputs = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            self.inputs = []\n",
    "            if args.langs == 'all':\n",
    "                langs = os.listdir(args.data_dir)\n",
    "            else:\n",
    "                langs = [args.langs]\n",
    "\n",
    "            data=[]\n",
    "            for lang in langs:\n",
    "                datafile = os.path.join(args.data_dir, lang, file_type+'.pkl')\n",
    "                if file_type == 'train':\n",
    "                    logger.warning(\"Creating features from dataset file at %s\", datafile)\n",
    "                # with open(datafile) as f:\n",
    "                #     data.extend([json.loads(x)['code'] for idx,x in enumerate(f.readlines()) if idx%world_size==local_rank])\n",
    "                dataset = pickle.load(open(datafile, 'rb'))\n",
    "                data.extend(['<s> '+' '.join(x['function'].split())+' </s>' for idx,x in enumerate(dataset) if idx%world_size==local_rank])\n",
    "\n",
    "            # random.shuffle(data)\n",
    "            data = data\n",
    "            length = len(data)\n",
    "            logger.warning(\"Data size: %d\"%(length))\n",
    "            input_ids = []\n",
    "            for idx,x in enumerate(data):\n",
    "                try:\n",
    "                    input_ids.extend(tokenizer.encode(x))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if idx % (length//10) == 0:\n",
    "                    percent = idx / (length//10) * 10\n",
    "                    logger.warning(\"Rank %d, load %d\"%(local_rank, percent))\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "            length = len(input_ids)\n",
    "            for i in range(0, length-block_size, block_size):\n",
    "                self.inputs.append(input_ids[i : i + block_size])            \n",
    "            del input_ids\n",
    "            gc.collect()\n",
    "\n",
    "            if file_type == 'train':\n",
    "                logger.warning(\"Rank %d Training %d token, %d samples\"%(local_rank, length, len(self.inputs)))\n",
    "                logger.warning(\"Saving features into cached file %s\", cached_file)\n",
    "            with open(cached_file, 'wb') as handle:\n",
    "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.inputs[item])\n",
    "\n",
    "class finetuneDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, logger, file_type='train', block_size=1024):\n",
    "        if args.local_rank==-1:\n",
    "            local_rank=0\n",
    "            world_size=1\n",
    "        else:\n",
    "            local_rank=args.local_rank\n",
    "            world_size=torch.distributed.get_world_size()\n",
    "\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "        cached_file = os.path.join(args.output_dir, file_type+\"_blocksize_%d\"%(block_size)+\"_wordsize_%d\"%(world_size)+\"_rank_%d\"%(local_rank))\n",
    "        if os.path.exists(cached_file) and not args.overwrite_cache:\n",
    "            if file_type == 'train':\n",
    "                logger.warning(\"Loading features from cached file %s\", cached_file)\n",
    "            with open(cached_file, 'rb') as handle:\n",
    "                self.inputs = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            self.inputs = []\n",
    "\n",
    "            datafile = os.path.join(args.data_dir, f\"{file_type}.txt\")\n",
    "            if file_type == 'train':\n",
    "                logger.warning(\"Creating features from dataset file at %s\", datafile)\n",
    "            with open(datafile) as f:\n",
    "                data = f.readlines()\n",
    "\n",
    "            length = len(data)\n",
    "            logger.info(\"Data size: %d\"%(length))\n",
    "            input_ids = []\n",
    "            for idx,x in enumerate(data):\n",
    "                x = x.strip()\n",
    "                if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    x = \"<s> \" + x + \" </s>\"\n",
    "                try:\n",
    "                    input_ids.extend(tokenizer.encode(x))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if idx % (length//10) == 0:\n",
    "                    percent = idx / (length//10) * 10\n",
    "                    logger.warning(\"Rank %d, load %d\"%(local_rank, percent))\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "            length = len(input_ids) // world_size\n",
    "            logger.info(f\"tokens: {length*world_size}\")\n",
    "            input_ids = input_ids[local_rank*length: (local_rank+1)*length]\n",
    "\n",
    "            for i in range(0, length-block_size, block_size):\n",
    "                self.inputs.append(input_ids[i : i + block_size])            \n",
    "            del input_ids\n",
    "            gc.collect()\n",
    "\n",
    "            if file_type == 'train':\n",
    "                logger.warning(\"Rank %d Training %d token, %d samples\"%(local_rank, length, len(self.inputs)))\n",
    "                logger.warning(\"Saving features into cached file %s\", cached_file)\n",
    "            with open(cached_file, 'wb') as handle:\n",
    "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.inputs[item])\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, logger, file_type='train', block_size=1024):\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "        cached_file = os.path.join(args.output_dir, file_type+\"_blocksize_%d\"%(block_size))\n",
    "        if os.path.exists(cached_file) and not args.overwrite_cache:\n",
    "            with open(cached_file, 'rb') as handle:\n",
    "                self.inputs = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            self.inputs = []\n",
    "\n",
    "            datafile = os.path.join(args.data_dir, f\"{file_type}.txt\")\n",
    "            with open(datafile) as f:\n",
    "                data = f.readlines()\n",
    "\n",
    "            length = len(data)\n",
    "            logger.info(\"Data size: %d\"%(length))\n",
    "            input_ids = []\n",
    "            for idx,x in enumerate(data):\n",
    "                x = x.strip()\n",
    "                if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    x = \"<s> \" + x + \" </s>\"\n",
    "                try:\n",
    "                    input_ids.extend(tokenizer.encode(x))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if idx % (length//10) == 0:\n",
    "                    percent = idx / (length//10) * 10\n",
    "                    logger.warning(\"load %d\"%(percent))\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "            logger.info(f\"tokens: {len(input_ids)}\")\n",
    "            self.split(input_ids, tokenizer, logger, block_size=block_size)\n",
    "            del input_ids\n",
    "            gc.collect()\n",
    "\n",
    "            with open(cached_file, 'wb') as handle:\n",
    "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def split(self, input_ids, tokenizer, logger, block_size=1024):\n",
    "        sample = []\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            sample = input_ids[i: i+block_size]\n",
    "            if len(sample) == block_size:\n",
    "                for j in range(block_size):\n",
    "                    if tokenizer.convert_ids_to_tokens(sample[block_size-1-j])[0] == '\\u0120' or tokenizer.convert_ids_to_tokens(sample[block_size-1-j]).startswith(\"<NUM_LIT\"):\n",
    "                        break\n",
    "                    if sample[block_size-1-j] in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id]:\n",
    "                        if sample[block_size-1-j] != tokenizer.bos_token_id:\n",
    "                            j -= 1\n",
    "                        break\n",
    "                if j == block_size-1:\n",
    "                    print(tokenizer.decode(sample))\n",
    "                    exit()\n",
    "                sample = sample[: block_size-1-j]\n",
    "            # print(len(sample))\n",
    "            i += len(sample)\n",
    "            pad_len = block_size-len(sample)\n",
    "            sample += [tokenizer.pad_token_id]*pad_len\n",
    "            self.inputs.append(sample)\n",
    "\n",
    "            if len(self.inputs) % 10000 == 0:\n",
    "                logger.info(f\"{len(self.inputs)} samples\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.inputs[item])\n",
    "        \n",
    "\n",
    "\n",
    "class lineDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, logger, file_type='test', block_size=924):\n",
    "        datafile = os.path.join(args.data_dir, f\"{file_type}.json\")\n",
    "        with open(datafile) as f:\n",
    "            datas = f.readlines()\n",
    "\n",
    "        length = len(datas)\n",
    "        logger.info(\"Data size: %d\"%(length))\n",
    "        self.inputs = []\n",
    "        self.gts = []\n",
    "        for data in datas:\n",
    "            data = json.loads(data.strip())\n",
    "            self.inputs.append(tokenizer.encode(data[\"input\"])[-block_size:])\n",
    "            self.gts.append(data[\"gt\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.inputs[item]), self.gts[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"raw_data.json\", 'r') as f:\n",
    "    jsondata = json.load(f)\n",
    "\n",
    "data = jsondata['member']\n",
    "data.extend(jsondata['nonmember'])\n",
    "print(type(data), len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# This script generates CSV output reporting on the API Coverage of Terraform's\n",
      "# AWS Provider.\n",
      "#\n",
      "# In addition to Ruby, it depends on a properly configured Go development\n",
      "# environment with both terraform and aws-sdk-go present.\n",
      "#\n",
      "\n",
      "require 'csv'\n",
      "require 'json'\n",
      "require 'pathname'\n",
      "\n",
      "module APIs\n",
      "  module Terraform\n",
      "    def self.path\n",
      "      @path ||= Pathname(`go list -f '{{.Dir}}' github.com/hashicorp/terraform`.chomp)\n",
      "    end\n",
      "\n",
      "    def self.called?(api, op)\n",
      "      `git -C \"#{path}\" grep \"#{api}.*#{op}\" -- builtin/providers/aws | wc -l`.chomp.to_i > 0\n",
      "    end\n",
      "  end\n",
      "\n",
      "  module AWS\n",
      "    def self.path\n",
      "      @path ||= Pathname(`go list -f '{{.Dir}}' github.com/aws/aws-sdk-go/aws`.chomp).parent\n",
      "    end\n",
      "\n",
      "    def self.api_json_files\n",
      "      Pathname.glob(path.join('**', '*.normal.json'))\n",
      "    end\n",
      "\n",
      "    def self.each\n",
      "      api_json_files.each do |api_json_file|\n",
      "        json = JSON.parse(api_json_file.read)\n",
      "        api = api_json_file.dirname.basename\n",
      "        json[\"operations\"].keys.each do |op|\n",
      "          yield api, op\n",
      "        end\n",
      "      end\n",
      "    end\n",
      "  end\n",
      "end\n",
      "\n",
      "csv = CSV.new($stdout)\n",
      "csv << [\"API\", \"Operation\", \"Called in Terraform?\"]\n",
      "APIs::AWS.each do |api, op|\n",
      "  csv << [api, op, APIs::Terraform.called?(api, op)]\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(data[1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2050 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "for idx,x in enumerate(data):\n",
    "    x = x.strip()\n",
    "    if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "        pass\n",
    "    else:\n",
    "        x = \"<s> \" + x + \" </s>\"\n",
    "    try:\n",
    "        input_ids.extend(tokenizer.encode(x))\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "for idx,x in enumerate(data):\n",
    "    x = x.strip()\n",
    "    if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "        pass\n",
    "    else:\n",
    "        x = \"<s> \" + x + \" </s>\"\n",
    "    try:\n",
    "        input_ids.extend(tokenizer.encode(x))\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "for idx,x in enumerate(data):\n",
    "    if idx == 2: break\n",
    "    x = x.strip()\n",
    "    if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "        pass\n",
    "    else:\n",
    "        x = \"<s> \" + x + \" </s>\"\n",
    "    try:\n",
    "        input_ids.extend(tokenizer.encode(x))\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
